[{"authors":["admin"],"categories":null,"content":"Hi, I am now an Associate Professor with Multimedi Institute of Tianjin University. I received the B.E and Ph.D. degree from Shanghai Jiao Tong Univeristy under the supervision of Prof. Xiaokang Yang and Prof. Guangtao Zhai. I am also supervised by Prof. Chang Wen Chen when visiting at SUNY@Buffalo from 2014-2015. My research interest includes: image/video processing and cross-modal video content analysis.\nI have contributed to deep learning methods for solving artificial intelligence challenges in real-world applications, including image enhancement, anomaly detection, recommendation systems, and cross modal retrieval. My research has been published in top conferences and journals, including TIP, TCSVT, TMM, DSP, CVPR, and IFTC .Our group is supported by the National Natural Science Foundation of China in 2023、the Tianjin Natural Science Foundation, and the National Natural Science Foundation 2021 .\n天津大学多媒体组（TJUMMG）中文简介 Laboratory Introduction Tianjin University Multimedia Group was founded by Dr. Liu Jing and is affiliated to the School of Electrical Automation and Information Engineering of Tianjin University. The main goal is to enhance the understanding of multimedia through computer training. The laboratory is equipped with large models such as deep learning to improve the perceptual quality of digital multimedia content, so as to accurately identify and classify objects of interest to users and respond to the observed content. This laboratory belongs to the Institute of Television and Image Information, Ministry of Education, Tianjin University.Students who apply for my laboratory are free to choose their preferred research direction. At each stage of study, tutors will have in-depth discussions with students on recent scientific research progress, future direction planning, project promotion problems, etc., to help students tackle difficulties on the road to scientific research. Overcome difficulties.\nResearch Direction In recent years, the laboratory has hosted or participated in projects such as the National Natural Science Foundation, the Ministry of Science and Technology\u0026rsquo;s key R\u0026amp;D plans, and enterprise cooperation. Each research direction is supported by provincial, ministerial or national projects. To view the specific research directions of the laboratory, please click here.\nMaster\u0026rsquo;s Student Recruitment The master\u0026rsquo;s students to be recruited this year mainly participate in the Natural Science Foundation and general projects. Their main work is writing algorithms, building platforms, software simulation, etc., which requires students to have high software design capabilities and hands-on skills. Applicants are generally required to master the relevant knowledge of data structures and algorithms, and have the ability to select appropriate algorithms to solve problems according to specific problem characteristics; at the same time, they have relevant experience in software programming during the undergraduate study stage, and can be relatively proficient in programming languages such as: C, Python ,Matlab etc. Applicants who have participated in the Robotics Competition, the National College Student Electronic Design Competition, the College Student Software Design Competition, and the Big Data Challenge at the undergraduate level and won awards will be given priority. Interested students please contact the person in charge of the laboratory.\nOpenings We are looking for self-motivated graduate students working with me. For prospective students, please send your resume and transcript to my email.\n课题组招收2024年秋季入学硕士研究生，欢迎保研或考研同学邮件联系!\n","date":1704067200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1704067200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/jing-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jing-liu/","section":"authors","summary":"Hi, I am now an Associate Professor with Multimedi Institute of Tianjin University. I received the B.E and Ph.D. degree from Shanghai Jiao Tong Univeristy under the supervision of Prof.","tags":null,"title":"Jing Liu","type":"authors"},{"authors":["wangchengzhi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2014852cf0c2cb4f213fcaed40d1b2cd","permalink":"/author/chengzhi-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chengzhi-wang/","section":"authors","summary":"","tags":null,"title":"Chengzhi Wang","type":"authors"},{"authors":["wanghan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ead57a4c21684f4ae921137fb18695ec","permalink":"/author/han-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/han-wang/","section":"authors","summary":"","tags":null,"title":"Han Wang","type":"authors"},{"authors":["jianghao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"88fb64a0fdfaf6a8d4907011c2ffa612","permalink":"/author/hao-jiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hao-jiang/","section":"authors","summary":"","tags":null,"title":"Hao Jiang","type":"authors"},{"authors":["zhangjiaqi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"99859f57a65dc4927520769705efec93","permalink":"/author/jiaqi-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaqi-zhang/","section":"authors","summary":"","tags":null,"title":"Jiaqi Zhang","type":"authors"},{"authors":["wangjiaxiang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"13326f73b2f3173e57695377f69e5113","permalink":"/author/jiaxiang-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaxiang-wang/","section":"authors","summary":"","tags":null,"title":"Jiaxiang Wang","type":"authors"},{"authors":["maojingjun"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"46f584a1f1daafdc2f2fc03a881f985f","permalink":"/author/jingjun-mao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingjun-mao/","section":"authors","summary":"","tags":null,"title":"Jingjun Mao","type":"authors"},{"authors":["liujingrui"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fd7bbb928cc36195c9686ffb1515ad7a","permalink":"/author/jingrui-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingrui-liu/","section":"authors","summary":"","tags":null,"title":"Jingrui Liu","type":"authors"},{"authors":["sunlele"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f28ecb7af8716a02f5e16e80b3893c21","permalink":"/author/lele-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lele-sun/","section":"authors","summary":"","tags":null,"title":"Lele Sun","type":"authors"},{"authors":["shanglitao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ab6c62fbd16c94405722061fdeadad9d","permalink":"/author/litao-shang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/litao-shang/","section":"authors","summary":"","tags":null,"title":"Litao Shang","type":"authors"},{"authors":["yuanmin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5335a45b8f319e41ff8f7e8199268aa2","permalink":"/author/min-yuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/min-yuan/","section":"authors","summary":"","tags":null,"title":"Min Yuan","type":"authors"},{"authors":["renminjie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"45c457e530bc5f9e84275dbf08d8048d","permalink":"/author/minjie-ren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/minjie-ren/","section":"authors","summary":"","tags":null,"title":"Minjie Ren","type":"authors"},{"authors":["douqianqian"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8efbfff967f98bbf5d8cdbf0c13093b","permalink":"/author/qianqian-dou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qianqian-dou/","section":"authors","summary":"","tags":null,"title":"Qianqian Dou","type":"authors"},{"authors":["liqingying"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0d12899a6f468dbb9a483fc32bfc0836","permalink":"/author/qingying-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qingying-li/","section":"authors","summary":"","tags":null,"title":"Qingying Li","type":"authors"},{"authors":["marui"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0aef57c01ddd116c70ff46ccd031bf11","permalink":"/author/rui-ma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rui-ma/","section":"authors","summary":"","tags":null,"title":"Rui Ma","type":"authors"},{"authors":["wangweikang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6b3dd4c4dbbc79233b4e9ec543758f69","permalink":"/author/weikang-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/weikang-wang/","section":"authors","summary":"","tags":null,"title":"Weikang Wang","type":"authors"},{"authors":["mixiaofeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"750662a95a5667b7aa6ad97402322878","permalink":"/author/xiaofeng-mi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaofeng-mi/","section":"authors","summary":"","tags":null,"title":"Xiaofeng Mi","type":"authors"},{"authors":["lixin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4d33cfd3fc9c22bc700d4c8ca8d0a0c3","permalink":"/author/xin-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xin-li/","section":"authors","summary":"","tags":null,"title":"Xin Li","type":"authors"},{"authors":["wenxin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0b15c5303e275ee19c05b08241aef0e8","permalink":"/author/xin-wen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xin-wen/","section":"authors","summary":"","tags":null,"title":"Xin Wen","type":"authors"},{"authors":["jixinyu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1d7b3f07f800b691a68180327d89f177","permalink":"/author/xinyu-ji/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xinyu-ji/","section":"authors","summary":"","tags":null,"title":"Xinyu Ji","type":"authors"},{"authors":["huangxinyue"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a37fc441eeb1c9f097cae17e6faf9940","permalink":"/author/xinyue-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xinyue-huang/","section":"authors","summary":"","tags":null,"title":"Xinyue Huang","type":"authors"},{"authors":["fengyangbo"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"97de89bd93c9fe8559ed529402db77d7","permalink":"/author/yangbo-feng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yangbo-feng/","section":"authors","summary":"","tags":null,"title":"Yangbo Feng","type":"authors"},{"authors":["fuyujie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0d1ad1cf4542928b97ab173bff3177e4","permalink":"/author/yujie-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yujie-fu/","section":"authors","summary":"","tags":null,"title":"Yujie Fu","type":"authors"},{"authors":["gangyuxin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"43105a6cc9247911fe32b36bbb8d9e64","permalink":"/author/yuxin-gang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuxin-gang/","section":"authors","summary":"","tags":null,"title":"Yuxin Gang","type":"authors"},{"authors":["fanzhiwei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"349d52f325a2b48ce23632715b5ffc88","permalink":"/author/zhiwei-fan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiwei-fan/","section":"authors","summary":"","tags":null,"title":"Zhiwei Fan","type":"authors"},{"authors":["hezhuo"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"94c0ec38a6ff3c6ac918cd1b1b9e944f","permalink":"/author/zhuo-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhuo-he/","section":"authors","summary":"","tags":null,"title":"Zhuo He","type":"authors"},{"authors":["huangzijian"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"52c0de75fe6a1465a29b66459e71a497","permalink":"/author/zijian-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zijian-huang/","section":"authors","summary":"","tags":null,"title":"Zijian Huang","type":"authors"},{"authors":["yangziwen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dfc02775598976c66d01cb17d7224d90","permalink":"/author/ziwen-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ziwen-yang/","section":"authors","summary":"","tags":null,"title":"Ziwen Yang","type":"authors"},{"authors":["zhangzongbing"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f957da6d0d069a7cd3370aacf92f8ca3","permalink":"/author/zongbing-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zongbing-zhang/","section":"authors","summary":"","tags":null,"title":"Zongbing Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\nOnline courses Project or software documentation Tutorials The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"负责人简介 刘婧，博士，天津大学电气自动化与信息工程学院的副教授。毕业于上海交通大学，师从 杨小康教授和 翟广涛教授，获得了学士学位和博士学位。2014-2015年访问SUNY@Buffalo期间，导师是 陈长汶 教授。研究领域包括图像/视频处理和跨模态视频内容分析。研究发表在顶级会议和期刊上，包括TIP、TCSVT、TMM、DSP、CVPR和IFTC。项目申请于2023年获得了国家自然科学基金的资助。曾任国家重点研发计划“主流价值观协同传播与推荐技术”协调人，天津市自然科学基金项目“视频图像比特深度上转换研究”负责人，国家自然科学基金项目“基于自然图像特征的比特深度上转换关键技术研究”负责人。\n工作经历 2019-至今: 副教授, 天津大学\n2017-2019 : 助理教授, 天津大学\n2017-2019 : 博士后, 天津大学\n教育经历 2019.12-至今 天津大学，电气自动化与信息工程学院， 电子信息工程系， 副教授\n2017.03-2019.12天津大学，电气自动化与信息工程学院， 电子信息工程系， 讲师\n2014.12-2015.12 美国纽约州立大学布法罗分校，计算机科学与工程学院，联合培养\n2011.09-2017.03 上海交通大学，电子信息与电气工程学院， 博士\n2007.09-2011.07上海交通大学，电子信息与电气工程学院， 学士 ​\n研究兴趣 图像/视频处理（增强、质量评估等）\n视频内容分析（分割、跟踪、跨模态检索等）\n推荐系统（跨域推荐器、交互式推荐器）\n主要科研项目 国家重点研发计划课题，“主流价值观内容协同传播与推荐技术”，联系人、统筹执行人\n天津市自然科学基金项目，“视频图像比特深度上转换研究”，负责人\n人工智能教育部重点实验室课题，“面向多源数据的微表情识别技术研究”，负责人\n国家自然科学基金项目, “基于自然图像特性的比特深度上转换关键技术研究”，负责人\n博士后特别资助项目,“基于生成对抗网络的比特深度上转换关键技术研究”，负责人\n博士后面上项目,“基于编码器-解码器网络的比特深度上转换研究”， 负责人\n学术论著 大数据处理及搜索引擎实践，伊诺科学出版社，2020. （副主编）\n主要学术成就、奖励及荣誉 2021: 天津市科技进步特等奖\n2018：中国图象图形学学会优秀博士学位论文提名奖\n2020：天津市研究生科研创新项目指导教师\n2019：“北洋学者-青年骨干教师”人才计划\n2019：本科生毕业设计优秀指导教授\n主要讲授课程 2019至今，科学计算语言（本科生选修课）\n2018至今，大数据分析（全英文，本科生专业核心课）\n2019至今，多媒体智能分析与计算（研究生选修课）\n2019至今，大数据分析高级教程（全英文，面向留学生）\n实验室简介 天津大学多媒体组由刘婧博士创立，隶属于天津大学电气自动化与信息工程学院。主要目标为通过计算机训练对多媒体进行增强理解，实验室内配备深度学习等大模型以提高数字多媒体内容的感知质量，从而准确识别和分类使用者感兴趣的对象，并对观察的内容进行响应。本实验室属于天津大学教育部电视与图像信息研究所。报考我实验室的同学可以充分自由的选择喜好的研究方向，在每个学习阶段导师都会与学生针对近期科研进度、未来方向规划、项目推进难题等方面进行深度讨论，帮助学生在科研道路上攻坚克难。\n研究方向 本实验室近年来主持或参与国家自然科学基金、科技部重点研发计划、企业合作等项目，各研究方向均有省部级或国家级项目支持，查看实验室具体研究方向请 点击此处。\n硕士生招募 本年度拟招收的硕士研究生主要参与自然科学基金和面上项目，主要工作是编写算法、搭建平台、软件仿真等，对学生的软件设计能力和动手能力要求较高。一般要求申请人能够掌握数据结构及算法的相关知识，具备根据具体问题特征选择合适算法解决问题的能力；同时在本科学习阶段有软件编程相关经验，能够较为熟练的掌握编程语言如：C，Python，Matlab等。在本科阶段参与机器人大赛，全国大学生电子设计竞赛，大学生软件设计大赛，大数据挑战赛并获奖的申请人将会被优先考虑。请各位有兴趣的同学联系实验室负责人。\n其他（社会兼职等） 学术组织 ​ 中国计算机学会多媒体专委会委员\n​ 中国图象图形学学会多媒体专委会委员\n国际期刊 ​ Elservier期刊Displays(IF: 4.3，JCR Q1) 副主编\n​ Springer期刊Multimedia Tools and Applications(IF: 3.6)专刊 特约编辑\n国际会议 ​ Co-Chair——CVPR2016 Workshop on Computer Vision for Microscopy Image Analysis\n​ Organizational Committee——CIHW2022\n​ Program Committee Member——ACM MM2019－2023\n审稿 ​ IEEE TPAMI、NNLS、TIP、TMM、TCSVT、ECCV、ACM MM等\n","date":1711843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711843200,"objectID":"478c306d8fbc97412365252952a914c5","permalink":"/post/liujing-cn/","publishdate":"2024-03-31T00:00:00Z","relpermalink":"/post/liujing-cn/","section":"post","summary":"负责人简介 刘婧，博士，天津大学电气自动化与信息工程学院的副教","tags":null,"title":"天津大学多媒体组","type":"post"},{"authors":null,"categories":null,"content":"","date":1711843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711843200,"objectID":"504454b6b08e21f4db909c18d6986331","permalink":"/post/research-direction/","publishdate":"2024-03-31T00:00:00Z","relpermalink":"/post/research-direction/","section":"post","summary":"","tags":null,"title":"研究方向","type":"post"},{"authors":["Jing Liu","Le-Le Sun, Weizhi Nie, Peiguang Jing, Yuting Su"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"2926b447b214675984608b446ce5790e","permalink":"/publication/graph-disentangled-contrastive-learning-with-personalized-transfer-for-cross-domain-recommendation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/graph-disentangled-contrastive-learning-with-personalized-transfer-for-cross-domain-recommendation/","section":"publication","summary":"Cross-Domain Recommendation (CDR) has been proven to efectively alleviate the data sparsity problem in Recommender System (RS). Recent CDR methods often disentangle user features into domain-invariant and domain-specifc features for efcient cross-domain knowledge transfer. Despite showcasing robust performance, three crucial aspects remain unexplored for existing disentangled CDR approaches:i) The signifcance nuances of the interaction behaviors are ignored in generating disentangled features; ii) The user features are disentangled irrelevant to the individual items to be recommended; iii) The general knowledge transfer overlooks the user's personality when interacting with diverse items. To this end, we propose a Graph Disentangled Contrastive framework for CDR (GDCCDR) with personalized transfer by meta-networks. An adaptive parameter-free flter is proposed to gauge the signifcance of diverse interactions, thereby facilitating more refned disentangled representations. In sight of the success of Contrastive Learning (CL) in RS, we propose two CL-based constraints for item-aware disentanglement. Proximate CL ensures the coherence of domain-invariant features between domains, while eliminatory CL strives to disentangle features within each domains using mutual information between users and items. Finally, for domain-invariant features, we adopt meta-networks to achieve personalized transfer. Experimental results on four real-world datasets demonstrate the superiority of GDCCDR over state-of-the-art methods.Liu, Jing, Le-Le Sun, Weizhi Nie, Peiguang Jing, Yuting Su:Graph Disentangled Contrastive Learning with Personalized Transfer for Cross-Domain Recommendation. Proceedings of the AAAI Conference on Artificial Intelligence.Vol. 38. No. 8. 2024. ","tags":null,"title":"Graph Disentangled Contrastive Learning with Personalized Transfer for Cross-Domain Recommendation","type":"publication"},{"authors":["Jing Liu","Le-Le Sun, Weizhi Nie, Yuting Su, Yongdong Zhang, An-An Liu"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"72bc0f42c1e3a6ffc9c7debc4814d6d2","permalink":"/publication/inter-and-intra-domain-potential-user-preferences-for-cross-domain-recommendation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/inter-and-intra-domain-potential-user-preferences-for-cross-domain-recommendation/","section":"publication","summary":"Data sparsity poses a persistent challenge in Recommender Systems (RS), driving the emergence of Cross-Domain Recommendation (CDR) as a potential remedy. However, most existing CDR methods often struggle to circumvent the transfer of domain-specific information, which are perceived as noise in the target domain. Additionally, they primarily concentrate on inter-domain information transfer, disregarding the comprehensive exploration of data within intra-domains. To address these limitations, we propose SUCCDR (Separating User features with Compound samples), a novel approach that tackles data sparsity by leveraging both cross-domain knowledge transfer and comprehensive intra-domain analysis. Specifically, to ensure the exclusion of noisy domain-specific features during the transfer process, user preferences are separated into domain-invariant and domain-specific features through three efficient constraints. Furthermore, the unobserved items are leveraged to generate compound samples that intelligently merge observed and unobserved potential user-item interaction, utilizing a simple yet efficient attention mechanism to enable a comprehensive and unbiased representation of user preferences. We evaluate the performance of SUCCDR on two real-world datasets, Douban and Amazon, and compare it with state-of-the-art single-domain and cross-domain recommendation methods. The experimental results demonstrate that SUCCDR outperforms existing approaches, highlighting its ability to effectively alleviate data sparsity problem.Jing Liu,Le-Le Sun,Weizhi Nie,Yuting Su,Yongdong Zhang,An-An Liu:Inter- and Intra-Domain Potential User Preferences for Cross-Domain Recommendation.IEEE Transactions on Multimedia.IEEE Transactions on Multimedia, 2024. ","tags":null,"title":"Inter- and Intra-Domain Potential User Preferences for Cross-Domain Recommendation","type":"publication"},{"authors":["Zijian Chen, Wei Sun, Jun Jia, Fangfang Lu, Zicheng Zhang","Jing Liu","Ru Huang, Xiongkuo Min, Guangtao Zhai"],"categories":null,"content":"","date":1701216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701216000,"objectID":"872194a80e8ab4f02b6db282b72e3b1c","permalink":"/publication/band-2k-banding-artifact-noticeable-database-for-banding-detection-and-quality-assessment/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/band-2k-banding-artifact-noticeable-database-for-banding-detection-and-quality-assessment/","section":"publication","summary":"Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms. As undesirable artifacts, banding destroys the original image structure, thus degrading users' quality of experience (QoE). In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e. mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality ratings. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts. A dual convolutional neural network is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters. Experiments demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.Zijian Chen, Wei Sun, Jun Jia, Fangfang Lu, Zicheng Zhang, Jing Liu, Ru Huang, Xiongkuo Min, Guangtao Zhai:BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment. IEEE Transactions on Circuits and Systems for Video Technology.","tags":null,"title":"BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment","type":"publication"},{"authors":["Weikang Wang","Jing Liu","Yuting Su, Weizhi Nie"],"categories":null,"content":"","date":1698710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698710400,"objectID":"84c1cfda8a1a2f6c0096c689c0d57ada","permalink":"/publication/efficient-spatio-temporal-video-grounding-with-semantic-guided-feature-decomposition/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/efficient-spatio-temporal-video-grounding-with-semantic-guided-feature-decomposition/","section":"publication","summary":"Spatio-temporal video grounding (STVG) aims to localize the spatio-temporal object tube in a video according to a given text query. Current approaches address the STVG task with end-to-end frameworks while suffering from heavy computational complexity and insufficient spatio-temporal interactions. To overcome these limitations, we propose a novel Semantic-Guided Feature Decomposition based Network (SGFDN). A semantic-guided mapping operation is proposed to decompose the 3D spatio-temporal feature into 2D motions and 1D object embedding without losing much object-related semantic information. Thus, the computational complexity in computationally expensive operations such as attention mechanisms can be effectively reduced by replacing the input spatio-temporal feature with the decomposed features. Furthermore, based on this decomposition strategy, a pyramid relevance filtering based attention is proposed to capture the cross-modal interactions at multiple spatio-temporal scales. In addition, a decomposition-based grounding head is proposed to locate the queried objects with less computational complexity. Extensive experiments on two widely-used STVG datasets (VidSTG and HC-STVG) demonstrate that our method enjoys state-of-the-art performance as well as less computational complexity. Weikang Wang, Jing Liu, Yuting Su, Weizhi Nie:Efficient Spatio-Temporal Video Grounding with Semantic-Guided Feature Decomposition. ACM Multimedia (MM). 4867--4876 (2023).","tags":null,"title":"Efficient Spatio-Temporal Video Grounding with Semantic-Guided Feature Decomposition","type":"publication"},{"authors":null,"categories":null,"content":"Abstract: ​\tSpatio-temporal video grounding (STVG) aims to localize the spatiotemporal object tube in a video according to a given text query. Current approaches address the STVG task with end-to-end frameworks while suffering from heavy computational complexity and insufficient spatio-temporal interactions. To overcome these limitations, we propose a novel Semantic-Guided Feature Decomposition based Network (SGFDN). A semantic-guided mapping operation is proposed to decompose the 3D spatio-temporal feature into 2D motions and 1D object embedding without losing much object-related semantic information. Thus, the computational complexity in computationally expensive operations such as attention mechanisms can be effectively reduced by replacing the input spatio-temporal feature with the decomposed features. Furthermore, based on this decomposition strategy, a pyramid relevance filtering based attention is proposed to capture the crossmodal interactions at multiple spatio-temporal scales. In addition, a decomposition-based grounding head is proposed to locate the queried objects with less computational complexity. Extensive experiments on two widely-used STVG datasets (VidSTG and HCSTVG) demonstrate that our method enjoys state-of-the-art performance as well as less computational complexity.\n","date":1697155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697155200,"objectID":"6b5304574662919c0141136cb89e5fdf","permalink":"/project/efficient-spatio-temporal-video-grounding-with/","publishdate":"2023-10-13T00:00:00Z","relpermalink":"/project/efficient-spatio-temporal-video-grounding-with/","section":"project","summary":"patio-temporal video grounding (STVG) aims to localize the spatiotemporal object tube in a video according to a given text query.....","tags":["Video Content Analysis"],"title":"Efficient Spatio-Temporal Video Grounding with Semantic-Guided Feature Decomposition","type":"project"},{"authors":null,"categories":null,"content":"Abstract: ​\tFor video bit-depth enhancement (VBDE) tasks, inter-frame information is critical for removing false contours and recovering the details in low bit-depth (LBD) videos. However, due to different structural distortions and complex motions in the neighboring frames, it is difficult to effectively utilized inter-frame information. Most algorithms rely on alignment operations to provide information of neighboring frames, suffering from slow inference speed due to the complex alignment module design. Meanwhile, most existing methods sequentially perform the intra-frame feature extractions and inter-frame information fusions, but fail to efficiently fuse spatio-temporal information. Therefore, in this paper, we propose a two-stage progressive group (TSPG) network to find complementary information related to the target frame without adopting an alignment operation. To simultaneously achieve intra-frame feature extractions and inter-frame feature fusions, we propose a parallel spatio-temporal fusion (PSTF) module with a dual-branch spatial-temporal residual (DSTR) block to focus on more useful temporal information while ensuring a faster inference speeds. Extensive experiments on public datasets demonstrate that our proposed multi-stage spatio-temporal fusion network (named MSTFN) can quickly and effectively eliminate false contours and recover high quality target frames. Furthermore, our method outperforms the state-of-the-art methods in terms of both PSNR and SSIM, and can reach faster inference speeds.\n","date":1689465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689465600,"objectID":"3beaff4434effc571829e21c0802fe7e","permalink":"/project/multi-stage-spatio-temporal-fusion-network-for-fast-and-accurate-video-bit-depth-enhancement/","publishdate":"2023-07-16T00:00:00Z","relpermalink":"/project/multi-stage-spatio-temporal-fusion-network-for-fast-and-accurate-video-bit-depth-enhancement/","section":"project","summary":"For video bit-depth enhancement (VBDE) tasks, inter-frame information is critical for removing false contours and recovering the details in low bit-depth (LBD) video, .....","tags":["Imagevideo Processing"],"title":"Multi-stage Spatio-Temporal Fusion Network for Fast and Accurate Video Bit-depth Enhancement","type":"project"},{"authors":["Jing Liu","Zhiwei Fan, Ziwen Yang, Yuting Su, Xiaokang Yang"],"categories":null,"content":"","date":1689465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689465600,"objectID":"a7d20bca4454dc86e5d8f878274d444a","permalink":"/publication/multi-stage-spatio-temporal-fusion-network-for-fast-and-accurate-video-bit-depth-enhancement/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/publication/multi-stage-spatio-temporal-fusion-network-for-fast-and-accurate-video-bit-depth-enhancement/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Video bit-depth enhancement"],"title":"Multi-stage Spatio-Temporal Fusion Network for Fast and Accurate Video Bit-depth Enhancement","type":"publication"},{"authors":null,"categories":null,"content":"Abstract: ​\tOwing to its inherently dynamic nature and economical training cost, offline reinforcement learning (RL) is typically employed to implement an interactive recommender system (IRS). A crucial challenge in offline RL-based IRSs is the data sparsity issue, i.e. , it is hard to mine user preferences well from the limited number of user-item interactions. In this paper, we propose a knowledge-enhanced causal reinforcement learning model (KCRL) to mitigate data sparsity in IRSs. We make technical extensions to the offline RL framework in terms of the reward function and state representation. Specifically, we first propose a group preference-injected causal user model (GCUM) to learn user satisfaction ( i.e. , reward) estimation. We introduce beneficial group preference information, namely, the group effect, via causal inference to compensate for incomplete user interests extracted from sparse data. Then, we learn the RL recommendation policy with the reward given by the GCUM. We propose a knowledge-enhanced state encoder (KSE) to generate knowledge-enriched user state representations at each time step, which is assisted by a self-constructed user-item knowledge graph. Extensive experimental results on real-world datasets demonstrate that our model significantly outperforms the baselines.\n","date":1684195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684195200,"objectID":"49ebdbbb9faf27e5f35b6b8c28215858","permalink":"/project/knowledge-enhanced-causal-reinforcement-learning-model-for-interactive-recommendation/","publishdate":"2023-05-16T00:00:00Z","relpermalink":"/project/knowledge-enhanced-causal-reinforcement-learning-model-for-interactive-recommendation/","section":"project","summary":"Owing to its inherently dynamic nature and economical training cost, offline reinforcement learning (RL) is typically employed to implement an interactive recommender system (IRS)......","tags":["Recommendation Systems"],"title":"Knowledge-Enhanced Causal Reinforcement Learning Model for Interactive Recommendation","type":"project"},{"authors":["Weizhi Nie, Xin Wen","Jing Liu","Jiawei Chen, Jiancan Wu, Guoqing Jin, Jing Lu, An-An Liu"],"categories":null,"content":"","date":1684195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684195200,"objectID":"dd9b0803290162d9b5b38f37f08da519","permalink":"/publication/knowledge-enhanced-causal-reinforcement-learning-model-for-interactive-recommendation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/knowledge-enhanced-causal-reinforcement-learning-model-for-interactive-recommendation/","section":"publication","summary":"Owing to its inherently dynamic nature and economical training cost, offline reinforcement learning (RL) is typically employed to implement an interactive recommender system (IRS). A crucial challenge in offline RL-based IRSs is the data sparsity issue, i.e. , it is hard to mine user preferences well from the limited number of user-item interactions. In this paper, we propose a knowledge-enhanced causal reinforcement learning model (KCRL) to mitigate data sparsity in IRSs. We make technical extensions to the offline RL framework in terms of the reward function and state representation. Specifically, we first propose a group preference-injected causal user model (GCUM) to learn user satisfaction ( i.e. , reward) estimation. We introduce beneficial group preference information, namely, the group effect, via causal inference to compensate for incomplete user interests extracted from sparse data. Then, we learn the RL recommendation policy with the reward given by the GCUM. We propose a knowledge-enhanced state encoder (KSE) to generate knowledge-enriched user state representations at each time step, which is assisted by a self-constructed user-item knowledge graph. Extensive experimental results on real-world datasets demonstrate that our model significantly outperforms the baselines.Weizhi Nie, Xin Wen, Jing Liu, Jiawei Chen, Jiancan Wu, Guoqing Jin, Jing Lu, An-An Liu:Knowledge-Enhanced Causal Reinforcement Learning Model for Interactive Recommendation. IEEE Trans. Multim. 26：1129-1142 (2023).","tags":["Interactive recommender system"],"title":"Knowledge-Enhanced Causal Reinforcement Learning Model for Interactive Recommendation","type":"publication"},{"authors":["Minjie Ren, Xiangdong Huang","Jing Liu","Ming Liu, Xuanya Li, An-An Liu"],"categories":null,"content":"","date":1683504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683504000,"objectID":"9dc6894af00d3f57fd8aef3088b62911","permalink":"/publication/maln-multimodal-adversarial-learning-network-for-conversational-emotion-recognition/","publishdate":"2023-05-08T00:00:00Z","relpermalink":"/publication/maln-multimodal-adversarial-learning-network-for-conversational-emotion-recognition/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Emotion recognition in conversations"],"title":"MALN: Multimodal Adversarial Learning Network for Conversational Emotion Recognition","type":"publication"},{"authors":["Xin Wen, Weizhi Nie","Jing Liu","Yuting Su"],"categories":null,"content":"","date":1679443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679443200,"objectID":"c03c6ed24ed627a17d52c0b1eb680ae7","permalink":"/publication/mrft-multiscale-recurrent-fusion-transformer-based-prior-knowledge-for-bit-depth-enhancement/","publishdate":"2023-03-22T00:00:00Z","relpermalink":"/publication/mrft-multiscale-recurrent-fusion-transformer-based-prior-knowledge-for-bit-depth-enhancement/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Bit-depth enhancement"],"title":"MRFT: Multiscale Recurrent Fusion Transformer Based Prior Knowledge for Bit-Depth Enhancement","type":"publication"},{"authors":null,"categories":null,"content":"Abstract: ​\tComprehensive understanding of video content requires both spatial and temporal localization. However, there lacks a unified video action localization framework, which hinders the coordinated development of this field. Existing 3D CNN methods take fixed and limited input length at the cost of ignoring temporally long-range cross-modal interaction. On the other hand, despite having large temporal context, existing sequential methods often avoid dense cross-modal interactions for complexity reasons. To address this issue, in this paper, we propose a unified framework which handles the whole video in sequential manner with long-range and dense visual-linguistic interaction in an end-to-end manner. Specifically, a lightweight relevance filtering based transformer (Ref-Transformer) is designed, which is composed of relevance filtering based attention and temporally expanded MLP. The text-relevant spatial regions and temporal clips in video can be efficiently highlighted through the relevance filtering and then propagated among the whole video sequence with the temporally expanded MLP. Extensive experiments on three sub-tasks of referring video action localization, i.e., referring video segmentation, temporal sentence grounding, and spatiotemporal video grounding, show that the proposed framework achieves the state-of-the-art performance in all referring video action localization tasks. The code has been available at.\n","date":1676246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676246400,"objectID":"2fb00672775732f0dc61179d3728a156","permalink":"/project/sequence-as-a-whole-a-unified-framework-for-video-action-localization-with-long-range-text-query/","publishdate":"2023-02-13T00:00:00Z","relpermalink":"/project/sequence-as-a-whole-a-unified-framework-for-video-action-localization-with-long-range-text-query/","section":"project","summary":"Comprehensive understanding of video content requires both spatial and temporal localization. However, .....","tags":["Video Content Analysis"],"title":"Sequence as a Whole: A Unified Framework for Video Action Localization With Long-Range Text Query","type":"project"},{"authors":["Yuting Su, Weikang Wang","Jing Liu","Shuang Ma, Xiaokang Yang"],"categories":null,"content":"","date":1676246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676246400,"objectID":"2bb5503c4f604cd170ebc3cf610c3921","permalink":"/publication/sequence-as-a-whole-a-unified-framework-for-video-action-localization-with-long-range-text-query/","publishdate":"2023-02-13T00:00:00Z","relpermalink":"/publication/sequence-as-a-whole-a-unified-framework-for-video-action-localization-with-long-range-text-query/","section":"publication","summary":"Comprehensive understanding of video content requires both spatial and temporal localization. However, there lacks a unified video action localization framework, which hinders the coordinated development of this field. Existing 3D CNN methods take fixed and limited input length at the cost of ignoring temporally long-range cross-modal interaction. On the other hand, despite having large temporal context, existing sequential methods often avoid dense cross-modal interactions for complexity reasons. To address this issue, in this paper, we propose a unified framework which handles the whole video in sequential manner with long-range and dense visual-linguistic interaction in an end-to-end manner. Specifically, a lightweight relevance filtering based transformer (Ref-Transformer) is designed, which is composed of relevance filtering based attention and temporally expanded MLP. The text-relevant spatial regions and temporal clips in video can be efficiently highlighted through the relevance filtering and then propagated among the whole video sequence with the temporally expanded MLP. Extensive experiments on three sub-tasks of referring video action localization, i.e., referring video segmentation, temporal sentence grounding, and spatiotemporal video grounding, show that the proposed framework achieves the state-of-the-art performance in all referring video action localization tasks.Yuting Su, Weikang Wang, Jing Liu, Shuang Ma, Xiaokang Yang:Sequence as a Whole: A Unified Framework for Video Action Localization With Long-Range Text Query. IEEE Trans. Image Process. 32: 1403-1418 (2023)","tags":["Referring video segmentation"],"title":"Sequence as A Whole: A Unified Framework for Video Action Localization with Long-Range Text Query","type":"publication"},{"authors":["Han Wang","Jing Liu","Yuting Su, Xiaokang Yang"],"categories":null,"content":"","date":1672617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672617600,"objectID":"d1d572851d4433aaa1280588840d287c","permalink":"/publication/trajectory-guided-robust-visual-object-tracking-with-selective-remedy/","publishdate":"2023-01-02T00:00:00Z","relpermalink":"/publication/trajectory-guided-robust-visual-object-tracking-with-selective-remedy/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Visual object tracking"],"title":"Trajectory Guided Robust Visual Object Tracking With Selective Remedy","type":"publication"},{"authors":["Jing Liu","Ziwen Yang, Yuting Su, Xiaokang Yang"],"categories":null,"content":"","date":1632441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632441600,"objectID":"7aca64f59692717de4e8074a5a9d5526","permalink":"/publication/tanet-target-attention-network-for-video-bit-depth-enhancement/","publishdate":"2021-09-24T00:00:00Z","relpermalink":"/publication/tanet-target-attention-network-for-video-bit-depth-enhancement/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Video bit-depth enhancement"],"title":"TANet: Target Attention Network for Video Bit-Depth Enhancement","type":"publication"},{"authors":["Jing Liu","Xin Wen, Weizhi Nie, Yuting Su, Peiguang Jing, Xiaokang Yang"],"categories":null,"content":"","date":1626739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626739200,"objectID":"d48d9633bd4ccb5c894a020f6abb2f49","permalink":"/publication/residual-guided-multiscale-fusion-network-for-bit-depth-enhancement/","publishdate":"2021-07-20T00:00:00Z","relpermalink":"/publication/residual-guided-multiscale-fusion-network-for-bit-depth-enhancement/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Bit-depth enhancement"],"title":"Residual-Guided Multiscale Fusion Network for Bit-Depth Enhancement","type":"publication"},{"authors":["Peiguang Jing, Jing Zhang, Liqiang Nie, Shu Ye","Jing Liu","Yuting Su"],"categories":null,"content":"","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"43312d8794a84b3ce04d55caa1acbb3d","permalink":"/publication/tripartite-graph-regularized-latent-low-rank-representation-for-fashion-compatibility-prediction/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/publication/tripartite-graph-regularized-latent-low-rank-representation-for-fashion-compatibility-prediction/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Correlation"],"title":"Tripartite Graph Regularized Latent Low-Rank Representation for Fashion Compatibility Prediction","type":"publication"},{"authors":["Peiguang Jing, Yuechen Shang, Liqiang Nie, Yuting Su","Jing Liu","Meng Wang"],"categories":null,"content":"","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"5b099815005eca16dd5c2806954f795c","permalink":"/publication/learning-low-rank-sparse-representations-with-robust-relationship-inference-for-image-memorability-prediction/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/publication/learning-low-rank-sparse-representations-with-robust-relationship-inference-for-image-memorability-prediction/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Image memorability prediction"],"title":"Learning Low-Rank Sparse Representations With Robust Relationship Inference for Image Memorability Prediction","type":"publication"},{"authors":null,"categories":null,"content":"Abstract: ​\tThere is a growing demand for monitors to provide high-quality visualization with more bits representing each rendered pixel. However, since most existing images and videos are of low bit-depth (LBD), transforming LBD images to visually pleasant high bit-depth (HBD) versions is of significant value. Most existing bit-depth enhancement methods generate unsatisfactory HBD images with annoying false contour artifacts or blurry details, and some algorithms are also time-consuming. To overcome these drawbacks, we propose a bit-depth enhancement framework via concatenating all level features of deep neural networks (DNNs). A novel deep learning network is proposed based on the deep convolutional variational auto-encoders (VAEs), and skip connections that concatenate every two layers are applied to pass low-level and high-level features to consequent layers, easing the gradient vanishing problem. Meanwhile, the proposed network is optimized to generate the residual between original images and its quantized ones, which performs better than recovering HBD images directly. The experimental results show that the proposed algorithm can eliminate false contour artifacts of the recovered HBD images with low time consumption, and can achieve dramatic restoration performance gains compared with state-of-the-art methods both subjectively and objectively.\n","date":1557964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557964800,"objectID":"bf4a61f33df986568879785e651e40f2","permalink":"/project/be-calf-bit-depth-enhancement-by-concatenating-all-level-features-of-dnn/","publishdate":"2019-05-16T00:00:00Z","relpermalink":"/project/be-calf-bit-depth-enhancement-by-concatenating-all-level-features-of-dnn/","section":"project","summary":"There is a growing demand for monitors to provide high-quality visualization with more bits representing each rendered pixel. However, since most existing images and videos are of low bit-depth .....","tags":["Imagevideo Processing"],"title":"BE-CALF: Bit-Depth Enhancement by Concatenating All Level Features of DNN","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"}]